{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "Looks at the **k** shortest distances and classifies the each test set based on this distance.<br>\n",
    "Is called a **lazy learning** model because it makes no assumptions about the data.<br>\n",
    "Popular in recommender systems.<br><br>\n",
    "When there are multiple distance points, each record has multiple values, so we take the distance $d_{ij} $ where this is the difference between observation $i$ and $j$.<br><br>\n",
    "Distances must have **nonnegativity**, **self-proximity** ($d_{ii} = 0$), **symmetry** s.t. $d_{ij} = d_{ji}$, and **triangular inequality** s.t. $d_{ij1} + d_{ij2} \\geq d_{j1j2}$. <br><br>\n",
    "Common ways to measure include:<br>\n",
    "- Euclidean Distance: \n",
    "$$\n",
    "d_{ij} = \\sqrt{\\sum_{m=1}^{p} (x_{im} - x_{jm})^2}\n",
    "$$\n",
    "- Manhattan Distance:\n",
    "$$\n",
    "d_{ij} = \\sum_{m=1}^p |x_{im} - x_{jm}|\n",
    "$$\n",
    "- Maximum Coordinate Distance (Chebychev Distance)\n",
    "$$\n",
    "d_{ij} = \\max_{m=1,2,...,p} |x_{im} - x_{jm}|\n",
    "$$\n",
    "- Correlation-based similarity\n",
    "$$\n",
    "d_{ij} = 1 - r^2_{ij}\n",
    "$$\n",
    "- Canberra Distance:\n",
    "$$\n",
    "d_{ij} = \\sum_{m=1}^{p} \\frac{|x_{im} - x_{jm}|}{|x_{im} + x_{jm}|}\n",
    "$$\n",
    "- Cosine Similarity:\n",
    "$$\n",
    "d_{ij} = \\cos^{-1} \\left( \\frac{\\sum_{m=1}^p x_{im} x_{jm}}{\\sqrt{\\sum_{m=1}^p x_{im}^2} \\cdot \\sqrt{\\sum_{m=1}^p x_{jm}^2}} \\right)\n",
    "$$\n",
    "- Dice Similarity:\n",
    "$$\n",
    "d_{ij} = \\frac{2 \\cdot \\sum_{m=1}^p x_{im} x_{jm}}{\\sum_{m=1}^p x_{im} + \\sum_{m=1}^p x_{jm}}\n",
    "$$\n",
    "- Inner Product Similarity:\n",
    "$$\n",
    "d_{ij} = -\\sum_{m=1}^p x_{im} x_{jm}\n",
    "$$\n",
    "- Jaccard Similarity:\n",
    "$$\n",
    "d_{ij} = \\frac{\\sum_{m=1}^p x_{im} x_{jm}}{\\sum_{m=1}^p x_{im} + \\sum_{m=1}^p x_{jm} - \\sum_{m=1}^p x_{im} x_{jm}}\n",
    "$$\n",
    "- Max Product Similarity:\n",
    "$$\n",
    "d_{ij} = -\\max_{m=1,2,...,p} \\{x_{im} \\cdot x_{jm}\\}\n",
    "$$\n",
    "- Overlap Similarity:\n",
    "$$\n",
    "d_{ij} = \\frac{\\sum_{m=1}^p \\min(x_{im}, x_{jm})}{\\min\\left( \\sum_{m=1}^p x_{im}, \\sum_{m=1}^p x_{jm} \\right)}\n",
    "$$\n",
    "### Statistical Distance\n",
    "- Mahalanobis Distance:\n",
    "$$\n",
    "d_{ij} = \\sqrt{(x_i - \\mu_j) \\cdot S^{-1} \\cdot (x_i - \\mu_j)^T}\n",
    "$$\n",
    "\n",
    "-Measures the distance between a point and a distribution. <br>\n",
    "-First, split the data into subsets with the same classification. <br>\n",
    "-Find the average value $ \\mu_k $ for each attribute in a subset. <br>\n",
    "-Compute the inverse covariance matrix $ S^{-1} $ for the attributes within the subset. <br>\n",
    "-Use this matrix to scale distances, accounting for correlations and variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mahalanobis distance to the new point is 5.3345404887625145\n"
     ]
    }
   ],
   "source": [
    "distance_m = [\n",
    "    [64, 580, 29],\n",
    "    [66, 570, 33],\n",
    "    [68, 590, 37],\n",
    "    [69, 660, 46],\n",
    "    [73, 600, 55]\n",
    "]\n",
    "\n",
    "# Record to classify\n",
    "new = [66, 640, 44]\n",
    "\n",
    "# First take the average of each feature\n",
    "average = np.average(distance_m, axis=0)\n",
    "\n",
    "# Then find the covariance matrix\n",
    "cov_m = np.cov(distance_m, rowvar=False)\n",
    "\n",
    "# Now take the inverse\n",
    "cinv_m = np.linalg.inv(cov_m)\n",
    "\n",
    "# Now we take the difference between the new point and the average\n",
    "x_minus_mew = new - average\n",
    "\n",
    "# And transpose it\n",
    "x_minus_mew_T = x_minus_mew.T\n",
    "\n",
    "# Multiply and square root all the scalar factors\n",
    "d_m = np.sqrt(np.matmul(np.matmul(x_minus_mew, cinv_m), x_minus_mew_T))\n",
    "\n",
    "print(\"The Mahalanobis distance to the new point is\", d_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure binary data, create a table for each pair of records and keep track of the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[3, 1]\n"
     ]
    }
   ],
   "source": [
    "# Take the following two records, record one is apple and two is banana\n",
    "# The features are sphere, sweet, sour, cruncy\n",
    "binary_m = [\n",
    "    [1, 1, 1, 1],\n",
    "    [0, 1, 0, 0]\n",
    "]\n",
    "\n",
    "# The x axis is where j = 0 then 1, the y is where i = 0 then 1\n",
    "counts = [[0, 0], [0, 0]]\n",
    "for e in range(len(binary_m[0])):\n",
    "    counts[binary_m[0][e]][binary_m[1][e]] += 1\n",
    "\n",
    "for row in counts:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use simply metrics as listed here to calculate distance:\n",
    "- Kulczynski Similarity\n",
    "$$\n",
    "\\frac{d}{b+c}\n",
    "$$\n",
    "\n",
    "- RusselRao Similarity\n",
    "$$\n",
    "\\frac{d}[n]\n",
    "$$\n",
    "\n",
    "- Simple Matching Similarity\n",
    "$$\n",
    "\\frac{a+d}{n}\n",
    "$$\n",
    "\n",
    "- Rogers Tanimoto Similarity\n",
    "$$\n",
    "\\frac{a+d}{a+2*(b+c)+d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the distance between records in polynomial variables, we create a matrix where the rows are the features in record i and the columns are the features in record j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big thing to keep in mind when recording distance is relativity. Yes are the points close but to what scale? Take the following two pairs for example and calculate their euclidean distance.<br><br>\n",
    "Pair A: (500, $40,000) and (600, $40,000)<br>\n",
    "Pair B: (500, $40,000) and (500, $39,800)<br><br>\n",
    "At first glance, B is closer because the scale of 200 off in the second feature is much close than 100 in the first, but if the numbers were normalized it may be much easier to spot the winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "One common method for normalization is to **Z Tranform** the points, this is where everything is centered around the mean, which is transformed to 0, and the points lie between +- 3 standard deviations.<br>\n",
    "The formal for each point is\n",
    "$$\n",
    "\\frac{X_{im} - \\mu_m}{\\sigma_m}\n",
    "$$\n",
    "<br>There also exist **Range Transformations** to keep the points between a maximum and a minimum.\n",
    "$$\n",
    "\\frac{X_{im} - \\text{min}}{\\text{max} - \\text{min}}\n",
    "$$\n",
    "<br>To normalize in terms of 1, you can perform a **Proportion Transformation**, where the value of X becomes a total or percantage of the sum:\n",
    "$$\n",
    "\\frac{X_{im}}{\\sum_j|X_{jm}|}\n",
    "$$\n",
    "\n",
    "Lastly, you can transform the point based on the distance by the closeness of points, or the quantiles of data in which it exists in. This is known as a Interquartile Transformation, where the value of $ X_{im} $ is rescaled according to the first quantile , the second, and the third. Note that Q3 - Q1 is known as the inter-quartile range (IQR).\n",
    "\n",
    "\n",
    "This transformation is very strong at avoiding outliers, as it keeps measurements tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many Neighbours to Choose?\n",
    "#### Under-Smoothing vs Over-Smoothing\n",
    "- Low values of *k* (1, 3, ...) capture local structure in data, but can also absorb some noise.\n",
    "- High values of *k* provide more smoothing, and therefore less noise, can miss the local structure.\n",
    "- Interestingly if *k* = *n*, the \"naive rule\" is applied.\n",
    "- **Typically:** k < 20, and an odd number is chosen to avoid ties.\n",
    "- We use a test set set to find the best value for k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting\n",
    "- It can be argued that the closest neighbours have more influence on the current points\n",
    "- Weights $ (w_i) $ can be assigned to each neighbour that satisfy the following:\n",
    "1. Proportions to the distance of the test data point should stay the same\n",
    "2. The sum of all weights should be equal to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "**The Good**\n",
    "- Simple model\n",
    "- Good performance for a large dataset\n",
    "- Useful for predictions and classifications\n",
    "\n",
    "<br>**The Bad**\n",
    "- Black box (hard to explain the reasons for classifications)\n",
    "- Easy to overfit if validation set is not used\n",
    "- Can be computationally intensive\n",
    "- Need lots of data, which can lead to high levels of dimensionality\n",
    "     - You can reduce this with PCA\n",
    "     - Settle for \"almost nearest neighbours\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
